{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment 1 - Regression and Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"HA1-DatasetScaled.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-2.5469</td>\n",
       "      <td>-1.8316</td>\n",
       "      <td>-1.9110</td>\n",
       "      <td>-1.2742</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-2.5469</td>\n",
       "      <td>-1.8316</td>\n",
       "      <td>-1.9110</td>\n",
       "      <td>-1.2742</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-2.5469</td>\n",
       "      <td>-1.8316</td>\n",
       "      <td>-1.9110</td>\n",
       "      <td>-1.2742</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>41.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-2.5469</td>\n",
       "      <td>-1.8316</td>\n",
       "      <td>-1.9110</td>\n",
       "      <td>-1.2742</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-1.5994</td>\n",
       "      <td>-0.9850</td>\n",
       "      <td>-1.0041</td>\n",
       "      <td>-0.5006</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9684</td>\n",
       "      <td>-1.0090</td>\n",
       "      <td>-0.8995</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "0             -2.1737           -2.5469               -1.8316   \n",
       "1             -2.1737           -2.5469               -1.8316   \n",
       "2             -2.1737           -2.5469               -1.8316   \n",
       "3             -2.1737           -2.5469               -1.8316   \n",
       "4             -2.1737           -1.5994               -0.9850   \n",
       "\n",
       "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "0            -1.9110                -1.2742              -3.2043   \n",
       "1            -1.9110                -1.2742              -3.2043   \n",
       "2            -1.9110                -1.2742              -3.2043   \n",
       "3            -1.9110                -1.2742              -3.2043   \n",
       "4            -1.0041                -0.5006              -3.2043   \n",
       "\n",
       "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "0                  -2.6712            -2.1218                -1.2523   \n",
       "1                  -2.6712            -2.1218                -1.2523   \n",
       "2                  -2.6712            -2.1218                -1.2523   \n",
       "3                  -2.6712            -2.1218                -1.2523   \n",
       "4                  -2.6712            -2.1218                -1.2523   \n",
       "\n",
       "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
       "0          -2.2206  ...            0.7124         0.9014             0.8055   \n",
       "1          -2.2206  ...            0.7124         0.9014             0.8055   \n",
       "2          -2.2206  ...            0.7124         0.9014             0.8055   \n",
       "3          -2.2206  ...            0.7124         0.9014             0.8055   \n",
       "4          -2.2206  ...           -0.9684        -1.0090            -0.8995   \n",
       "\n",
       "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
       "0          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "1          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "2          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "3          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "4          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "\n",
       "   std_Valence  wtd_std_Valence  critical_temp  \n",
       "0      -1.7297          -1.4782           52.0  \n",
       "1      -1.7297          -1.4782           50.0  \n",
       "2      -1.7297          -1.4782           41.5  \n",
       "3      -1.7297          -1.4782           32.0  \n",
       "4      -1.7297          -1.4782           29.0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if any column has a missing value overall (not per row)\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2: Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SPLITTING #\n",
    "\n",
    "# Split data into training and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.drop('critical_temp', axis=1), df['critical_temp'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# X_train, y_train: Training data\n",
    "# X_test, y_test: Test data\n",
    "# X_val, y_val: Validation data (IVS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective 1** - Produce the best regression model for critical_temp (Dependent Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model perform GridSearch on testing set to establish the best hyperparameters min_sample_leaf and max_depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "40 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\tree\\_classes.py\", line 1247, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'criterion' parameter of DecisionTreeRegressor must be a str among {'absolute_error', 'squared_error', 'friedman_mse', 'poisson'}. Got 'mse' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\tree\\_classes.py\", line 1247, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'criterion' parameter of DecisionTreeRegressor must be a str among {'absolute_error', 'squared_error', 'friedman_mse', 'poisson'}. Got 'mae' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [          nan           nan           nan           nan           nan\n",
      "           nan           nan           nan           nan           nan\n",
      "           nan           nan           nan           nan           nan\n",
      "           nan           nan           nan           nan           nan\n",
      " -410.9496523  -410.9496523  -410.9496523  -410.9496523  -370.5121146\n",
      " -370.5121146  -370.5121146  -370.5121146  -333.04056799 -333.04056799\n",
      " -333.60816167 -333.60816167 -282.21612104 -282.22493727 -280.96077694\n",
      " -281.31381682 -244.70345908 -244.2339327  -244.55987584 -245.13321791\n",
      "           nan           nan           nan           nan           nan\n",
      "           nan           nan           nan           nan           nan\n",
      "           nan           nan           nan           nan           nan\n",
      "           nan           nan           nan           nan           nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Combine train and test data for GridSearchCV\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "# Create an array of indices:\n",
    "# -1 indicates the sample is part of the training set\n",
    "# 0 indicates the sample is part of the test set (used for evaluation during GridSearch)\n",
    "test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_test.shape[0])]\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# Define the regressor\n",
    "dtree_regressor = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {\n",
    "    'max_depth': np.arange(2, 7),\n",
    "    'criterion': ['mse', 'friedman_mse', 'mae'],\n",
    "    'min_samples_leaf': [5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Run the grid search using the predefined split\n",
    "gridCV = GridSearchCV(dtree_regressor, parameters, scoring=scorer, cv=ps)\n",
    "\n",
    "# Fitting the grid search\n",
    "gridCV.fit(X_combined, y_combined)\n",
    "\n",
    "# Get the best estimator\n",
    "dtree_regressor_best = gridCV.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the results as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antun\\AppData\\Local\\Temp\\ipykernel_14704\\3225674998.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_mse['mean_test_score'] = -df_mse['mean_test_score']  # Convert negative MSE to positive\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max Depth</th>\n",
       "      <th>Criterion</th>\n",
       "      <th>Min Samples Leaf</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>10</td>\n",
       "      <td>244.233933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>20</td>\n",
       "      <td>244.559876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>5</td>\n",
       "      <td>244.703459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>25</td>\n",
       "      <td>245.133218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>20</td>\n",
       "      <td>280.960777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>25</td>\n",
       "      <td>281.313817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>5</td>\n",
       "      <td>282.216121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>10</td>\n",
       "      <td>282.224937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>10</td>\n",
       "      <td>333.040568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>5</td>\n",
       "      <td>333.040568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>20</td>\n",
       "      <td>333.608162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>25</td>\n",
       "      <td>333.608162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>25</td>\n",
       "      <td>370.512115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>20</td>\n",
       "      <td>370.512115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>10</td>\n",
       "      <td>370.512115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>5</td>\n",
       "      <td>370.512115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>25</td>\n",
       "      <td>410.949652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>20</td>\n",
       "      <td>410.949652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>10</td>\n",
       "      <td>410.949652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>5</td>\n",
       "      <td>410.949652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Max Depth     Criterion  Min Samples Leaf         MSE\n",
       "37          6  friedman_mse                10  244.233933\n",
       "38          6  friedman_mse                20  244.559876\n",
       "36          6  friedman_mse                 5  244.703459\n",
       "39          6  friedman_mse                25  245.133218\n",
       "34          5  friedman_mse                20  280.960777\n",
       "35          5  friedman_mse                25  281.313817\n",
       "32          5  friedman_mse                 5  282.216121\n",
       "33          5  friedman_mse                10  282.224937\n",
       "29          4  friedman_mse                10  333.040568\n",
       "28          4  friedman_mse                 5  333.040568\n",
       "30          4  friedman_mse                20  333.608162\n",
       "31          4  friedman_mse                25  333.608162\n",
       "27          3  friedman_mse                25  370.512115\n",
       "26          3  friedman_mse                20  370.512115\n",
       "25          3  friedman_mse                10  370.512115\n",
       "24          3  friedman_mse                 5  370.512115\n",
       "23          2  friedman_mse                25  410.949652\n",
       "22          2  friedman_mse                20  410.949652\n",
       "21          2  friedman_mse                10  410.949652\n",
       "20          2  friedman_mse                 5  410.949652"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the results\n",
    "results = gridCV.cv_results_\n",
    "\n",
    "# Convert the results to a Pandas dataframe\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Extract each hyperparameter into its own column from the 'params' column\n",
    "df_results['max_depth'] = df_results['params'].apply(lambda x: x['max_depth'])\n",
    "df_results['criterion'] = df_results['params'].apply(lambda x: x['criterion'])\n",
    "df_results['min_samples_leaf'] = df_results['params'].apply(lambda x: x['min_samples_leaf'])\n",
    "\n",
    "# Select relevant columns and create the desired dataframe\n",
    "df_mse = df_results[['max_depth', 'criterion', 'min_samples_leaf', 'mean_test_score']]\n",
    "df_mse['mean_test_score'] = -df_mse['mean_test_score']  # Convert negative MSE to positive\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_mse.columns = ['Max Depth', 'Criterion', 'Min Samples Leaf', 'MSE']\n",
    "\n",
    "# Drop rows where 'Mean MSE' is NaN\n",
    "df_mse = df_mse.dropna(subset=['MSE'])\n",
    "\n",
    "# Display the dataframe by having it as the last statement\n",
    "df_mse.sort_values(by='MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of the GridSearch the best max_depth is 6 levels and the best min_sample_leaf is 10 samples per leaf. Now let's look at the metrics of the chosen model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on Testing Set: 233.11592998150303\n",
      "Root Mean Squared Error (RMSE) on Testing Set: 15.26813446304109\n",
      "Mean Absolute Error (MAE) on Testing Set: 10.145789994547487\n",
      "R-squared on Testing Set: 0.8037844098467851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predict on the testing set\n",
    "y_pred_test = dtree_regressor_best.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Mean Squared Error (MSE) on Testing Set: {mse_test}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Testing Set: {rmse_test}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Testing Set: {mae_test}\")\n",
    "print(f\"R-squared on Testing Set: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LINEAR REGRESSION MODEL #\n",
    "\n",
    "# Train model\n",
    "reg1 = LinearRegression().fit(X_train, y_train)\n",
    "# Predict on test set\n",
    "y_pred1_test = reg1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. RIDGE REGRESSION MODEL WITH CROSS-VALIDATION (K-Fold Cross Validation) #\n",
    "\n",
    "# Train model\n",
    "reg2 = RidgeCV(cv=5, alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X_train, y_train) # ridge regression with cross-validation and range of alpha values 0.001, 0.01, 0.1, 1\n",
    "# Predict on test set\n",
    "y_pred2_test = reg2.predict(X_test)\n",
    "# Resulting best alpha from cross-validation\n",
    "reg2_best_alpha = reg2.alpha_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2365.4204092714936, tolerance: 1268.5487813735356\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 361898.7478658757, tolerance: 1268.5487813735356\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9233.799768580124, tolerance: 1268.4192516819078\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 436203.90534183756, tolerance: 1268.4192516819078\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9606.876737037208, tolerance: 1270.1356089854016\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 469457.9665184091, tolerance: 1270.1356089854016\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10051.505139176268, tolerance: 1268.6594451461792\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 347207.61595434835, tolerance: 1268.6594451461792\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10572.823469202034, tolerance: 1266.4176161756648\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 267493.41052703466, tolerance: 1266.4176161756648\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.590e+05, tolerance: 1.586e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# 3. LASSO REGRESSION MODEL WITH CROSS-VALIDATION (K-Fold Cross Validation) #\n",
    "\n",
    "# Train model\n",
    "reg3 = LassoCV(cv=5, alphas=[1e-3, 1e-2, 1e-1, 1], max_iter=10000).fit(X_train, y_train) # lasso regression with cross-validation and range of alpha values 0.001, 0.01, 0.1, 1\n",
    "# Predict on test set\n",
    "y_pred3_test = reg3.predict(X_test)\n",
    "# Resulting best alpha from cross-validation\n",
    "reg3_best_alpha = reg3.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2027.2736570164561, tolerance: 1268.4192516819078\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2084.0813863184303, tolerance: 1270.1356089854016\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "# 4. LASSO REGRESSION MODEL WITH CROSS-VALIDATION (K-Fold Cross Validation) - With Normalised Features #\n",
    "\n",
    "# Compute statistics from the training set\n",
    "mean_train = X_train.mean()\n",
    "max_min_range_train = X_train.max() - X_train.min()\n",
    "# Normalize the training set\n",
    "X_train_norm = (X_train - mean_train) / max_min_range_train\n",
    "# Normalize the test set using the same statistics from the training set\n",
    "X_test_norm = (X_test - mean_train) / max_min_range_train\n",
    "\n",
    "# Train model\n",
    "reg4 = LassoCV(cv=5, alphas=[1e-3, 1e-2, 1e-1, 1], max_iter=10000).fit(X_train_norm, y_train) # lasso regression with cross-validation and range of alpha values 0.001, 0.01, 0.1, 1\n",
    "# Predict on test set\n",
    "y_pred4_test = reg4.predict(X_test_norm)\n",
    "# Resulting best alpha from cross-validation\n",
    "reg4_best_alpha = reg4.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2580.8678956055082, tolerance: 1268.5487813735356\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5715.567715616897, tolerance: 1268.4192516819078\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1570.0766816055402, tolerance: 1270.1356089854016\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1500.3447217349894, tolerance: 1266.4176161756648\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41076.09225651855, tolerance: 1268.5487813735356\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 47088.99569465127, tolerance: 1268.4192516819078\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 64963.90119346557, tolerance: 1270.1356089854016\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 46333.45135027636, tolerance: 1268.6594451461792\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33228.08889143681, tolerance: 1266.4176161756648\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 116652.91480490984, tolerance: 1268.5487813735356\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 217619.17534582643, tolerance: 1268.4192516819078\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 153175.18272516178, tolerance: 1270.1356089854016\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 105869.77442049002, tolerance: 1268.6594451461792\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 67485.5525735491, tolerance: 1266.4176161756648\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 334035.2302381224, tolerance: 1268.5487813735356\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 361824.2126848351, tolerance: 1268.4192516819078\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 379177.22275077854, tolerance: 1270.1356089854016\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 303323.5686409725, tolerance: 1268.6594451461792\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 336917.6537955892, tolerance: 1266.4176161756648\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2365.4204092714936, tolerance: 1268.5487813735356\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 361898.7478658757, tolerance: 1268.5487813735356\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9233.799768580124, tolerance: 1268.4192516819078\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 436203.90534183756, tolerance: 1268.4192516819078\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9606.876737037208, tolerance: 1270.1356089854016\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 469457.9665184091, tolerance: 1270.1356089854016\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10051.505139176268, tolerance: 1268.6594451461792\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 347207.61595434835, tolerance: 1268.6594451461792\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10572.823469202034, tolerance: 1266.4176161756648\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:617: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 267493.41052703466, tolerance: 1266.4176161756648\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.590e+05, tolerance: 1.586e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# 5. ELASTIC NET REGRESSION MODEL WiTH CROSS-VALIDATION (K-Fold Cross Validation) #\n",
    "\n",
    "# Train model\n",
    "reg5 = ElasticNetCV(cv=5, alphas=[1e-3, 1e-2, 1e-1, 1], l1_ratio=[.1, .5, .7, .9, .95, .99, 1], max_iter=10000).fit(X_train, y_train) # elastic net regression with cross-validation and range of alpha values 0.001, 0.01, 0.1, 1 as well as range of l1_ratio values 0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1\n",
    "# Predict on test set\n",
    "y_pred5_test = reg5.predict(X_test)\n",
    "# Resulting best alpha from cross-validation\n",
    "reg5_best_alpha = reg5.alpha_\n",
    "# Resulting best l1_ratio from cross-validation\n",
    "reg5_best_l1_ratio = reg5.l1_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: The warnings above are the result of models not being able to converge. This is due to the fact that several models with different hyperparameters were tested and for some of the parameters the model did not converge. The models that did not converge were not used in the final model selection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Best Alpha</th>\n",
       "      <th>Best l1_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>300.226879</td>\n",
       "      <td>17.327056</td>\n",
       "      <td>13.202890</td>\n",
       "      <td>0.740512</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>300.094027</td>\n",
       "      <td>17.323222</td>\n",
       "      <td>13.195956</td>\n",
       "      <td>0.740627</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>299.806594</td>\n",
       "      <td>17.314924</td>\n",
       "      <td>13.169824</td>\n",
       "      <td>0.740875</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso Regression (Normalised)</td>\n",
       "      <td>301.007264</td>\n",
       "      <td>17.349561</td>\n",
       "      <td>13.175865</td>\n",
       "      <td>0.739838</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ElasticNet Regression</td>\n",
       "      <td>299.806594</td>\n",
       "      <td>17.314924</td>\n",
       "      <td>13.169824</td>\n",
       "      <td>0.740875</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model         MSE       RMSE        MAE        R2  \\\n",
       "0              Linear Regression  300.226879  17.327056  13.202890  0.740512   \n",
       "1               Ridge Regression  300.094027  17.323222  13.195956  0.740627   \n",
       "2               Lasso Regression  299.806594  17.314924  13.169824  0.740875   \n",
       "3  Lasso Regression (Normalised)  301.007264  17.349561  13.175865  0.739838   \n",
       "4          ElasticNet Regression  299.806594  17.314924  13.169824  0.740875   \n",
       "\n",
       "  Best Alpha Best l1_ratio  \n",
       "0          -             -  \n",
       "1        0.1             -  \n",
       "2      0.001             -  \n",
       "3      0.001             -  \n",
       "4      0.001           1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data frame\n",
    "df_results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'MAE', 'R2', 'Best Alpha', 'Best l1_ratio'])\n",
    "# Create data frames for each model\n",
    "model1 = pd.DataFrame({'Model': 'Linear Regression', 'MSE': mean_squared_error(y_test, y_pred1_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred1_test)), 'MAE': mean_absolute_error(y_test, y_pred1_test), 'R2': r2_score(y_test, y_pred1_test), 'Best Alpha': '-', 'Best l1_ratio': '-'}, index=[0])\n",
    "model2 = pd.DataFrame({'Model': 'Ridge Regression', 'MSE': mean_squared_error(y_test, y_pred2_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred2_test)), 'MAE': mean_absolute_error(y_test, y_pred2_test), 'R2': r2_score(y_test, y_pred2_test), 'Best Alpha': reg2_best_alpha, 'Best l1_ratio': '-'}, index=[1])\n",
    "model3 = pd.DataFrame({'Model': 'Lasso Regression', 'MSE': mean_squared_error(y_test, y_pred3_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred3_test)), 'MAE': mean_absolute_error(y_test, y_pred3_test), 'R2': r2_score(y_test, y_pred3_test), 'Best Alpha': reg3_best_alpha, 'Best l1_ratio': '-'}, index=[2])\n",
    "model4 = pd.DataFrame({'Model': 'Lasso Regression (Normalised)', 'MSE': mean_squared_error(y_test, y_pred4_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred4_test)), 'MAE': mean_absolute_error(y_test, y_pred4_test), 'R2': r2_score(y_test, y_pred4_test), 'Best Alpha': reg4_best_alpha, 'Best l1_ratio': '-'}, index=[3])\n",
    "model5 = pd.DataFrame({'Model': 'ElasticNet Regression', 'MSE': mean_squared_error(y_test, y_pred5_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred5_test)), 'MAE': mean_absolute_error(y_test, y_pred5_test), 'R2': r2_score(y_test, y_pred5_test), 'Best Alpha': reg5_best_alpha, 'Best l1_ratio': reg5_best_l1_ratio}, index=[4])\n",
    "# Concatenate the data frames\n",
    "df_results = pd.concat([df_results, model1, model2, model3, model4, model5], ignore_index=True)\n",
    "# Print results\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REGRESSION MODEL SELECTION:** The Decision Tree Regression has lower error metrics (MSE, RMSE, and MAE), indicating more accurate predictions, and a higher R-squared (R2), suggesting a better fit to the data, making it a superior model. Therefore, we decided to go with the Decision Tree Regression model. Let's now test the model on independent validation set (IVS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Testing Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>228.452231</td>\n",
       "      <td>233.115930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>15.114636</td>\n",
       "      <td>15.268134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>9.959863</td>\n",
       "      <td>10.145790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-squared</th>\n",
       "      <td>0.809123</td>\n",
       "      <td>0.803784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Validation Set  Testing Set\n",
       "MSE            228.452231   233.115930\n",
       "RMSE            15.114636    15.268134\n",
       "MAE              9.959863    10.145790\n",
       "R-squared        0.809123     0.803784"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate metrics for validation set\n",
    "metrics_val = {\n",
    "    \"Validation Set\": [mse, rmse, mae, r2]\n",
    "}\n",
    "# Calculate metrics for testing set\n",
    "metrics_test = {\n",
    "    \"Testing Set\": [mse_test, rmse_test, mae_test, r2_test]\n",
    "}\n",
    "# Create DataFrames\n",
    "df_metrics_val = pd.DataFrame(metrics_val, index=[\"MSE\", \"RMSE\", \"MAE\", \"R-squared\"])\n",
    "df_metrics_test = pd.DataFrame(metrics_test, index=[\"MSE\", \"RMSE\", \"MAE\", \"R-squared\"])\n",
    "# Combine the DataFrames for validation and testing metrics\n",
    "df_metrics = pd.concat([df_metrics_val, df_metrics_test], axis=1)\n",
    "# Display the DataFrame\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CONCUSION:*** The metrics on the validation set are reasonably close to those on the test set which suggests that our model is performing consistently and is likely to generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective 2** - Produce the best binary classification model assuming as positive all instances with values of critical_temp >= 80.0 and as negatives all remaining cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Firstly, we need to transform all the target columns to the binary classification problem instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# transform the target column to a binary classification problem\n",
    "# 0 - negative\n",
    "# 1 - positive\n",
    "def transform_to_binary(x):\n",
    "    return 1 if x >= 80 else 0\n",
    "\n",
    "y_train_bin = y_train.apply(transform_to_binary)\n",
    "y_val_bin = y_val.apply(transform_to_binary)\n",
    "y_test_bin = y_test.apply(transform_to_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11307\n",
       "1     2244\n",
       "Name: critical_temp, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_bin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can see that the data is imbalanced, we have much more negative (0) observations. Therefore, we will use `F1 score` as our main metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 2.1 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_classification_scores(y_true, y_predicted):\n",
    "    return pd.DataFrame([\n",
    "        ['Accuracy', round(accuracy_score(y_true, y_predicted), 4)],\n",
    "        ['Precision', round(precision_score(y_true, y_predicted), 4)],\n",
    "        ['Recall', round(recall_score(y_true, y_predicted), 4)],\n",
    "        ['F1 score', round(f1_score(y_true, y_predicted), 4)]\n",
    "    ], columns=['metric', 'score'])\n",
    "\n",
    "def print_confusion_matrix(y_true, y_predicted):\n",
    "    print(\"The Confusion Matrix\")\n",
    "    print(pd.DataFrame(confusion_matrix(y_true, y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We will explore different values of the following hyperparameters: `min_samples_leaf`, `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>0.804788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0.804449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>0.803324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>0.802974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>0.801115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     min_samples_leaf  max_depth        F1\n",
       "457                 4         40  0.804788\n",
       "242                 6         22  0.804449\n",
       "482                 6         42  0.803324\n",
       "265                 4         24  0.802974\n",
       "481                 4         42  0.801115"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_samples_leaf_values = range(2,50,2)\n",
    "max_depth_values = range(2,50,2)\n",
    "\n",
    "results = {\"min_samples_leaf\": [], \"max_depth\": [], \"F1\": []}\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    for min_samples_leaf in min_samples_leaf_values:\n",
    "        model = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "        model.fit(X_train, y_train_bin)\n",
    "        y_preds = model.predict(X_test)\n",
    "        f1 = f1_score(y_test_bin, y_preds)\n",
    "\n",
    "        results[\"min_samples_leaf\"].append(min_samples_leaf)\n",
    "        results[\"max_depth\"].append(max_depth)\n",
    "        results[\"F1\"].append(f1)\n",
    "\n",
    "scores_df = pd.DataFrame(results, index=None)\n",
    "scores_df = scores_df.sort_values(by=\"F1\", ascending=False)\n",
    "\n",
    "best_max_depth = int(scores_df.iloc[0].max_depth)\n",
    "best_min_samples_leaf = int(scores_df.iloc[0].min_samples_leaf)\n",
    "\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Model with `max_depth`=6 And `min_samples_leaf`=24 turned out to be the best. However, if we look at the table below showing 5 best performing hyperparameters combinations, we can make a few interesting observations. \n",
    "The F1 scores for all those models are very similar and `min_samples_leaf`=6 seems to be a very stable value. However, when it comes to `max_depth`, we donâ€™t see one definite value. There is quite a big variation in this value among the models.\n",
    "Therefore, we decided to investigate `max_depth` value deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1_scores_train = []\n",
    "f1_scores_test = []\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=best_min_samples_leaf)\n",
    "    model.fit(X_train, y_train_bin)\n",
    "    y_preds_train = model.predict(X_train)\n",
    "    y_preds_test = model.predict(X_test)\n",
    "    f1_scores_train.append(f1_score(y_train_bin, y_preds_train))\n",
    "    f1_scores_test.append(f1_score(y_test_bin, y_preds_test))\n",
    "\n",
    "plt.plot(max_depth_values, f1_scores_train, label='training set')\n",
    "plt.plot(max_depth_values, f1_scores_test, label='test set')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Based on the plot above we can see that the upper bound for the F1 score on the test set stabilizes at approximately 0.78.. We already get almost the best possible results (on the test set) for `max_depth`~18 and increasing `max_depth` does not give us any significant improvements. Thus, we set `max_depth`=18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_final_model_scores(model):\n",
    "    model.fit(X_train, y_train_bin)\n",
    "\n",
    "    y_preds_test = model.predict(X_test)\n",
    "    test_set_scores_df = get_classification_scores(y_test_bin, y_preds_test).rename(columns={'score': 'test_set'})\n",
    "\n",
    "    y_preds_val = model.predict(X_val)\n",
    "    validation_set_scores_df = get_classification_scores(y_val_bin, y_preds_val).rename(columns={'score': 'validation_set'})\n",
    "\n",
    "    return pd.merge(test_set_scores_df, validation_set_scores_df, on='metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>test_set</th>\n",
       "      <th>validation_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.9324</td>\n",
       "      <td>0.9244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.8205</td>\n",
       "      <td>0.8320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.7509</td>\n",
       "      <td>0.7190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.7842</td>\n",
       "      <td>0.7714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric  test_set  validation_set\n",
       "0   Accuracy    0.9324          0.9244\n",
       "1  Precision    0.8205          0.8320\n",
       "2     Recall    0.7509          0.7190\n",
       "3   F1 score    0.7842          0.7714"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_max_depth = 18\n",
    "model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf)\n",
    "bn_decision_tree_final_scores_df = get_final_model_scores(model)\n",
    "bn_decision_tree_final_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 2.2. Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Baseline - default LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.8878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.6790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.5957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.6346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric   score\n",
       "0   Accuracy  0.8878\n",
       "1  Precision  0.6790\n",
       "2     Recall  0.5957\n",
       "3   F1 score  0.6346"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train_bin)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "get_classification_scores(y_test_bin, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\antun\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>solver</th>\n",
       "      <th>penalty</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>10.00</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.00</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>None</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.10</td>\n",
       "      <td>newton-cholesky</td>\n",
       "      <td>None</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.01</td>\n",
       "      <td>newton-cholesky</td>\n",
       "      <td>None</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.01</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>None</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        c           solver penalty        F1\n",
       "54  10.00        liblinear      l1  0.670300\n",
       "44   1.00        newton-cg    None  0.666667\n",
       "33   0.10  newton-cholesky    None  0.666667\n",
       "20   0.01  newton-cholesky    None  0.666667\n",
       "18   0.01        newton-cg    None  0.666667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "solver_and_penalty = {'lbfgs': ['l2', None], 'liblinear': ['l1', 'l2'], 'newton-cg': ['l2', None],\n",
    "                      'newton-cholesky': ['l2', None], 'sag': ['l2', None], 'saga': ['l1', 'l2', None]}\n",
    "\n",
    "results = {\"c\": [], \"solver\": [], \"penalty\": [], \"F1\": []}\n",
    "\n",
    "for c in c_values:\n",
    "    for solver in solver_and_penalty.keys():\n",
    "        for penalty in solver_and_penalty[solver]:\n",
    "            model = LogisticRegression(C=c, solver=solver, penalty=penalty)\n",
    "            model.fit(X_train, y_train_bin)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            f1 = f1_score(y_test_bin, y_pred_test)\n",
    "\n",
    "            results['c'].append(c)\n",
    "            results['solver'].append(solver)\n",
    "            results['penalty'].append(penalty)\n",
    "            results['F1'].append(f1)\n",
    "\n",
    "scores_df = pd.DataFrame(results, index=None)\n",
    "scores_df = scores_df.sort_values(by=\"F1\", ascending=False)\n",
    "\n",
    "best_c = scores_df.iloc[0].c # any\n",
    "best_solver = scores_df.iloc[0].solver # newton-cg/newton-cholesky\n",
    "best_penalty = scores_df.iloc[0].penalty # None\n",
    "\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>test_set</th>\n",
       "      <th>validation_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.6746</td>\n",
       "      <td>0.7331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.6661</td>\n",
       "      <td>0.6511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.6703</td>\n",
       "      <td>0.6897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric  test_set  validation_set\n",
       "0   Accuracy    0.8929          0.8961\n",
       "1  Precision    0.6746          0.7331\n",
       "2     Recall    0.6661          0.6511\n",
       "3   F1 score    0.6703          0.6897"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=best_c, solver=best_solver, penalty=best_penalty)\n",
    "bn_linear_model_final_scores_df = get_final_model_scores(model)\n",
    "bn_linear_model_final_scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
