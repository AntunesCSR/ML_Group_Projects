{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment 1 - Regression and Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"HA1-DatasetScaled.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "      <th>critical_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-2.5469</td>\n",
       "      <td>-1.8316</td>\n",
       "      <td>-1.9110</td>\n",
       "      <td>-1.2742</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-2.5469</td>\n",
       "      <td>-1.8316</td>\n",
       "      <td>-1.9110</td>\n",
       "      <td>-1.2742</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-2.5469</td>\n",
       "      <td>-1.8316</td>\n",
       "      <td>-1.9110</td>\n",
       "      <td>-1.2742</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>41.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-2.5469</td>\n",
       "      <td>-1.8316</td>\n",
       "      <td>-1.9110</td>\n",
       "      <td>-1.2742</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7124</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.1737</td>\n",
       "      <td>-1.5994</td>\n",
       "      <td>-0.9850</td>\n",
       "      <td>-1.0041</td>\n",
       "      <td>-0.5006</td>\n",
       "      <td>-3.2043</td>\n",
       "      <td>-2.6712</td>\n",
       "      <td>-2.1218</td>\n",
       "      <td>-1.2523</td>\n",
       "      <td>-2.2206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9684</td>\n",
       "      <td>-1.0090</td>\n",
       "      <td>-0.8995</td>\n",
       "      <td>-3.3123</td>\n",
       "      <td>-2.7858</td>\n",
       "      <td>-1.6406</td>\n",
       "      <td>-1.5235</td>\n",
       "      <td>-1.7297</td>\n",
       "      <td>-1.4782</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "0             -2.1737           -2.5469               -1.8316   \n",
       "1             -2.1737           -2.5469               -1.8316   \n",
       "2             -2.1737           -2.5469               -1.8316   \n",
       "3             -2.1737           -2.5469               -1.8316   \n",
       "4             -2.1737           -1.5994               -0.9850   \n",
       "\n",
       "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "0            -1.9110                -1.2742              -3.2043   \n",
       "1            -1.9110                -1.2742              -3.2043   \n",
       "2            -1.9110                -1.2742              -3.2043   \n",
       "3            -1.9110                -1.2742              -3.2043   \n",
       "4            -1.0041                -0.5006              -3.2043   \n",
       "\n",
       "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "0                  -2.6712            -2.1218                -1.2523   \n",
       "1                  -2.6712            -2.1218                -1.2523   \n",
       "2                  -2.6712            -2.1218                -1.2523   \n",
       "3                  -2.6712            -2.1218                -1.2523   \n",
       "4                  -2.6712            -2.1218                -1.2523   \n",
       "\n",
       "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
       "0          -2.2206  ...            0.7124         0.9014             0.8055   \n",
       "1          -2.2206  ...            0.7124         0.9014             0.8055   \n",
       "2          -2.2206  ...            0.7124         0.9014             0.8055   \n",
       "3          -2.2206  ...            0.7124         0.9014             0.8055   \n",
       "4          -2.2206  ...           -0.9684        -1.0090            -0.8995   \n",
       "\n",
       "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
       "0          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "1          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "2          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "3          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "4          -3.3123              -2.7858        -1.6406            -1.5235   \n",
       "\n",
       "   std_Valence  wtd_std_Valence  critical_temp  \n",
       "0      -1.7297          -1.4782           52.0  \n",
       "1      -1.7297          -1.4782           50.0  \n",
       "2      -1.7297          -1.4782           41.5  \n",
       "3      -1.7297          -1.4782           32.0  \n",
       "4      -1.7297          -1.4782           29.0  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if any column has a missing value overall (not per row)\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2: Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df.drop('critical_temp', axis=1), df['critical_temp'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# X_train, y_train: Training data\n",
    "# X_test, y_test: Test data\n",
    "# X_val, y_val: Validation data (IVS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective 1** - Produce the best regression model for critical_temp (Dependent Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model perform GridSearch on testing set to establish the best hyperparameters `min_sample_leaf` and `max_depth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test data for GridSearchCV\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "# Create an array of indices:\n",
    "# -1 indicates the sample is part of the training set\n",
    "# 0 indicates the sample is part of the test set (used for evaluation during GridSearch)\n",
    "test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_test.shape[0])]\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# Define the regressor\n",
    "dtree_regressor = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {\n",
    "    'max_depth': np.arange(2, 7),\n",
    "    'criterion': ['mse', 'friedman_mse', 'mae'],\n",
    "    'min_samples_leaf': [5, 10, 20, 25]\n",
    "}\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "gridCV = GridSearchCV(dtree_regressor, parameters, scoring=scorer, cv=ps)\n",
    "gridCV.fit(X_combined, y_combined)\n",
    "dtree_regressor_best = gridCV.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the results as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Max Depth     Criterion  Min Samples Leaf         MSE\n",
      "38          6  friedman_mse                20  260.543030\n",
      "39          6  friedman_mse                25  260.572988\n",
      "37          6  friedman_mse                10  260.715494\n",
      "36          6  friedman_mse                 5  264.353381\n",
      "33          5  friedman_mse                10  282.909458\n"
     ]
    }
   ],
   "source": [
    "results = gridCV.cv_results_\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Extract each hyperparameter into its own column from the 'params' column\n",
    "df_results['max_depth'] = df_results['params'].apply(lambda x: x['max_depth'])\n",
    "df_results['criterion'] = df_results['params'].apply(lambda x: x['criterion'])\n",
    "df_results['min_samples_leaf'] = df_results['params'].apply(lambda x: x['min_samples_leaf'])\n",
    "\n",
    "# Select relevant columns and create the desired dataframe\n",
    "df_mse = df_results[['max_depth', 'criterion', 'min_samples_leaf', 'mean_test_score']]\n",
    "df_mse['mean_test_score'] = -df_mse['mean_test_score']  # Convert negative MSE to positive\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_mse.columns = ['Max Depth', 'Criterion', 'Min Samples Leaf', 'MSE']\n",
    "\n",
    "# Drop rows where 'Mean MSE' is NaN\n",
    "df_mse = df_mse.dropna(subset=['MSE'])\n",
    "\n",
    "# Display the dataframe\n",
    "df_mse_sorted = df_mse.sort_values(by='MSE')\n",
    "df_mse_head = df_mse_sorted.head(5)\n",
    "print(df_mse_head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of the GridSearch the best `max_depth` is 6 levels and the best `min_sample_leaf` is 10 samples per leaf. Now let's look at the metrics of the chosen model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on Testing Set: 213.5520473386162\n",
      "Root Mean Squared Error (RMSE) on Testing Set: 14.613420110932834\n",
      "Mean Absolute Error (MAE) on Testing Set: 9.789836523772927\n",
      "R-squared on Testing Set: 0.819670802394632\n"
     ]
    }
   ],
   "source": [
    "# Predict on the testing set\n",
    "y_pred_test = dtree_regressor_best.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Mean Squared Error (MSE) on Testing Set: {mse_test}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) on Testing Set: {rmse_test}\")\n",
    "print(f\"Mean Absolute Error (MAE) on Testing Set: {mae_test}\")\n",
    "print(f\"R-squared on Testing Set: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LINEAR REGRESSION MODEL #\n",
    "reg1 = LinearRegression().fit(X_train, y_train)\n",
    "y_pred1_test = reg1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. RIDGE REGRESSION MODEL WITH CROSS-VALIDATION (K-Fold Cross Validation) #\n",
    "\n",
    "reg2 = RidgeCV(cv=5, alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X_train, y_train) # ridge regression with cross-validation and range of alpha values 0.001, 0.01, 0.1, 1\n",
    "y_pred2_test = reg2.predict(X_test)\n",
    "reg2_best_alpha = reg2.alpha_ # Resulting best alpha from cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LASSO REGRESSION MODEL WITH CROSS-VALIDATION (K-Fold Cross Validation) #\n",
    "\n",
    "reg3 = LassoCV(cv=5, alphas=[1e-3, 1e-2, 1e-1, 1], max_iter=10000).fit(X_train, y_train) # lasso regression with cross-validation and range of alpha values 0.001, 0.01, 0.1, 1\n",
    "y_pred3_test = reg3.predict(X_test)\n",
    "reg3_best_alpha = reg3.alpha_ # Resulting best alpha from cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LASSO REGRESSION MODEL WITH CROSS-VALIDATION (K-Fold Cross Validation) - With Normalised Features #\n",
    "\n",
    "# Compute statistics from the training set\n",
    "mean_train = X_train.mean()\n",
    "max_min_range_train = X_train.max() - X_train.min()\n",
    "# Normalize the training set\n",
    "X_train_norm = (X_train - mean_train) / max_min_range_train\n",
    "# Normalize the test set using the same statistics from the training set\n",
    "X_test_norm = (X_test - mean_train) / max_min_range_train\n",
    "\n",
    "reg4 = LassoCV(cv=5, alphas=[1e-3, 1e-2, 1e-1, 1], max_iter=10000).fit(X_train_norm, y_train) # lasso regression with cross-validation and range of alpha values 0.001, 0.01, 0.1, 1\n",
    "y_pred4_test = reg4.predict(X_test_norm)\n",
    "reg4_best_alpha = reg4.alpha_ # Resulting best alpha from cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ELASTIC NET REGRESSION MODEL WiTH CROSS-VALIDATION (K-Fold Cross Validation) #\n",
    "\n",
    "# Train model\n",
    "reg5 = ElasticNetCV(cv=5, alphas=[1e-3, 1e-2, 1e-1, 1], l1_ratio=[.1, .5, .7, .9, .95, .99, 1], max_iter=10000).fit(X_train, y_train) # elastic net regression with cross-validation and range of alpha values 0.001, 0.01, 0.1, 1 as well as range of l1_ratio values 0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1\n",
    "y_pred5_test = reg5.predict(X_test)\n",
    "reg5_best_alpha = reg5.alpha_  # Resulting best alpha from cross-validation\n",
    "reg5_best_l1_ratio = reg5.l1_ratio_ # Resulting best l1_ratio from cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: The warnings above are the result of models not being able to converge. This is due to the fact that several models with different hyperparameters were tested and for some of the parameters the model did not converge. The models that did not converge were not used in the final model selection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R2</th>\n",
       "      <th>Best Alpha</th>\n",
       "      <th>Best l1_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>318.608030</td>\n",
       "      <td>17.849595</td>\n",
       "      <td>13.618111</td>\n",
       "      <td>0.730959</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>318.598621</td>\n",
       "      <td>17.849331</td>\n",
       "      <td>13.617414</td>\n",
       "      <td>0.730967</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>318.649047</td>\n",
       "      <td>17.850744</td>\n",
       "      <td>13.595841</td>\n",
       "      <td>0.730924</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso Regression (Normalised)</td>\n",
       "      <td>320.048025</td>\n",
       "      <td>17.889886</td>\n",
       "      <td>13.598531</td>\n",
       "      <td>0.729743</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ElasticNet Regression</td>\n",
       "      <td>318.649047</td>\n",
       "      <td>17.850744</td>\n",
       "      <td>13.595841</td>\n",
       "      <td>0.730924</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model         MSE       RMSE        MAE        R2  \\\n",
       "0              Linear Regression  318.608030  17.849595  13.618111  0.730959   \n",
       "1               Ridge Regression  318.598621  17.849331  13.617414  0.730967   \n",
       "2               Lasso Regression  318.649047  17.850744  13.595841  0.730924   \n",
       "3  Lasso Regression (Normalised)  320.048025  17.889886  13.598531  0.729743   \n",
       "4          ElasticNet Regression  318.649047  17.850744  13.595841  0.730924   \n",
       "\n",
       "  Best Alpha Best l1_ratio  \n",
       "0          -             -  \n",
       "1       0.01             -  \n",
       "2      0.001             -  \n",
       "3      0.001             -  \n",
       "4      0.001           1.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'MAE', 'R2', 'Best Alpha', 'Best l1_ratio'])\n",
    "# Create data frames for each model\n",
    "model1 = pd.DataFrame({'Model': 'Linear Regression', 'MSE': mean_squared_error(y_test, y_pred1_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred1_test)), 'MAE': mean_absolute_error(y_test, y_pred1_test), 'R2': r2_score(y_test, y_pred1_test), 'Best Alpha': '-', 'Best l1_ratio': '-'}, index=[0])\n",
    "model2 = pd.DataFrame({'Model': 'Ridge Regression', 'MSE': mean_squared_error(y_test, y_pred2_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred2_test)), 'MAE': mean_absolute_error(y_test, y_pred2_test), 'R2': r2_score(y_test, y_pred2_test), 'Best Alpha': reg2_best_alpha, 'Best l1_ratio': '-'}, index=[1])\n",
    "model3 = pd.DataFrame({'Model': 'Lasso Regression', 'MSE': mean_squared_error(y_test, y_pred3_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred3_test)), 'MAE': mean_absolute_error(y_test, y_pred3_test), 'R2': r2_score(y_test, y_pred3_test), 'Best Alpha': reg3_best_alpha, 'Best l1_ratio': '-'}, index=[2])\n",
    "model4 = pd.DataFrame({'Model': 'Lasso Regression (Normalised)', 'MSE': mean_squared_error(y_test, y_pred4_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred4_test)), 'MAE': mean_absolute_error(y_test, y_pred4_test), 'R2': r2_score(y_test, y_pred4_test), 'Best Alpha': reg4_best_alpha, 'Best l1_ratio': '-'}, index=[3])\n",
    "model5 = pd.DataFrame({'Model': 'ElasticNet Regression', 'MSE': mean_squared_error(y_test, y_pred5_test), 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred5_test)), 'MAE': mean_absolute_error(y_test, y_pred5_test), 'R2': r2_score(y_test, y_pred5_test), 'Best Alpha': reg5_best_alpha, 'Best l1_ratio': reg5_best_l1_ratio}, index=[4])\n",
    "# Concatenate the data frames\n",
    "df_results = pd.concat([df_results, model1, model2, model3, model4, model5], ignore_index=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGRESSION MODEL SELECTION:\n",
    "The Decision Tree Regression has lower error metrics (MSE, RMSE, and MAE), indicating more accurate predictions, and a higher R-squared (R2), suggesting a better fit to the data, making it a superior model. Therefore, we decided to go with the Decision Tree Regression model. Let's now test the model on independent validation set (IVS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation Set</th>\n",
       "      <th>Testing Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>225.769588</td>\n",
       "      <td>213.552047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>15.025631</td>\n",
       "      <td>14.613420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>9.921410</td>\n",
       "      <td>9.789837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-squared</th>\n",
       "      <td>0.811364</td>\n",
       "      <td>0.819671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Validation Set  Testing Set\n",
       "MSE            225.769588   213.552047\n",
       "RMSE            15.025631    14.613420\n",
       "MAE              9.921410     9.789837\n",
       "R-squared        0.811364     0.819671"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on validation set - Decision Tree\n",
    "y_pred_val = dtree_regressor_best.predict(X_val)\n",
    "# Define metrics for validation set\n",
    "mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "r2_val = r2_score(y_val, y_pred_val)\n",
    "# Calculate metrics for validation set\n",
    "metrics_val = {\n",
    "    \"Validation Set\": [mse_val, rmse_val, mae_val, r2_val]\n",
    "}\n",
    "# Calculate metrics for testing set\n",
    "metrics_test = {\n",
    "    \"Testing Set\": [mse_test, rmse_test, mae_test, r2_test]\n",
    "}\n",
    "# Create DataFrames\n",
    "df_metrics_val = pd.DataFrame(metrics_val, index=[\"MSE\", \"RMSE\", \"MAE\", \"R-squared\"])\n",
    "df_metrics_test = pd.DataFrame(metrics_test, index=[\"MSE\", \"RMSE\", \"MAE\", \"R-squared\"])\n",
    "# Combine the DataFrames for validation and testing metrics\n",
    "df_metrics = pd.concat([df_metrics_val, df_metrics_test], axis=1)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***CONCUSION:*** The metrics on the validation set are reasonably close to those on the test set which suggests that our model is performing consistently and is likely to generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective 2** - Produce the best binary classification model assuming as positive all instances with values of critical_temp >= 80.0 and as negatives all remaining cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Firstly, we need to transform all the target columns to the binary classification problem instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# transform the target column to a binary classification problem: 0 - negative, 1 - positive\n",
    "def transform_to_binary(x):\n",
    "    return 1 if x >= 80 else 0\n",
    "\n",
    "y_train_bin = y_train.apply(transform_to_binary)\n",
    "y_val_bin = y_val.apply(transform_to_binary)\n",
    "y_test_bin = y_test.apply(transform_to_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "critical_temp\n",
       "0    11341\n",
       "1     2210\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_bin.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can see that the data is imbalanced, we have much more negative (0) observations. Therefore, we will use `F1 score` as our main metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 2.1 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_classification_scores(y_true, y_predicted):\n",
    "    return pd.DataFrame([\n",
    "        ['Accuracy', round(accuracy_score(y_true, y_predicted), 4)],\n",
    "        ['Precision', round(precision_score(y_true, y_predicted), 4)],\n",
    "        ['Recall', round(recall_score(y_true, y_predicted), 4)],\n",
    "        ['F1 score', round(f1_score(y_true, y_predicted), 4)]\n",
    "    ], columns=['metric', 'score'])\n",
    "\n",
    "def print_confusion_matrix(y_true, y_predicted):\n",
    "    print(\"The Confusion Matrix\")\n",
    "    print(pd.DataFrame(confusion_matrix(y_true, y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We will explore different values of the following hyperparameters: `min_samples_leaf`, `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "min_samples_leaf_values = range(2,50,2)\n",
    "max_depth_values = range(2,50,2)\n",
    "\n",
    "results = {\"min_samples_leaf\": [], \"max_depth\": [], \"F1\": []}\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    for min_samples_leaf in min_samples_leaf_values:\n",
    "        model = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "        model.fit(X_train, y_train_bin)\n",
    "        y_preds = model.predict(X_test)\n",
    "        f1 = f1_score(y_test_bin, y_preds)\n",
    "\n",
    "        results[\"min_samples_leaf\"].append(min_samples_leaf)\n",
    "        results[\"max_depth\"].append(max_depth)\n",
    "        results[\"F1\"].append(f1)\n",
    "\n",
    "scores_df = pd.DataFrame(results, index=None)\n",
    "scores_df = scores_df.sort_values(by=\"F1\", ascending=False)\n",
    "\n",
    "best_max_depth = int(scores_df.iloc[0].max_depth)\n",
    "best_min_samples_leaf = int(scores_df.iloc[0].min_samples_leaf)\n",
    "\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Model with `max_depth`=6 And `min_samples_leaf`=24 turned out to be the best. However, if we look at the table below showing 5 best performing hyperparameters combinations, we can make a few interesting observations. \n",
    "The F1 scores for all those models are very similar and `min_samples_leaf`=6 seems to be a very stable value. However, when it comes to `max_depth`, we don’t see one definite value. There is quite a big variation in this value among the models.\n",
    "Therefore, we decided to investigate `max_depth` value deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "f1_scores_train = []\n",
    "f1_scores_test = []\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=best_min_samples_leaf)\n",
    "    model.fit(X_train, y_train_bin)\n",
    "    y_preds_train = model.predict(X_train)\n",
    "    y_preds_test = model.predict(X_test)\n",
    "    f1_scores_train.append(f1_score(y_train_bin, y_preds_train))\n",
    "    f1_scores_test.append(f1_score(y_test_bin, y_preds_test))\n",
    "\n",
    "plt.plot(max_depth_values, f1_scores_train, label='training set')\n",
    "plt.plot(max_depth_values, f1_scores_test, label='test set')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Based on the plot above we can see that the upper bound for the F1 score on the test set stabilizes at approximately 0.78.. We already get almost the best possible results (on the test set) for `max_depth`~18 and increasing `max_depth` does not give us any significant improvements. Thus, we set `max_depth`=18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_final_model_scores(model, validation_set=False):\n",
    "    model.fit(X_train, y_train_bin)\n",
    "\n",
    "    y_preds_test = model.predict(X_test)\n",
    "    test_set_scores_df = get_classification_scores(y_test_bin, y_preds_test).rename(columns={'score': 'test_set'})\n",
    "\n",
    "    if validation_set:\n",
    "        y_preds_val = model.predict(X_val)\n",
    "        validation_set_scores_df = get_classification_scores(y_val_bin, y_preds_val).rename(columns={'score': 'validation_set'})\n",
    "        return pd.merge(test_set_scores_df, validation_set_scores_df, on='metric')\n",
    "    return test_set_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "best_max_depth = 18\n",
    "decision_tree_model = DecisionTreeClassifier(max_depth=best_max_depth, min_samples_leaf=best_min_samples_leaf)\n",
    "get_final_model_scores(decision_tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 2.2. Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "c_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "solver_and_penalty = {'lbfgs': ['l2', None], 'liblinear': ['l1', 'l2'], 'newton-cg': ['l2', None],\n",
    "                      'newton-cholesky': ['l2', None], 'sag': ['l2', None], 'saga': ['l1', 'l2', None]}\n",
    "\n",
    "results = {\"c\": [], \"solver\": [], \"penalty\": [], \"F1\": []}\n",
    "\n",
    "for c in c_values:\n",
    "    for solver in solver_and_penalty.keys():\n",
    "        for penalty in solver_and_penalty[solver]:\n",
    "            model = LogisticRegression(C=c, solver=solver, penalty=penalty)\n",
    "            model.fit(X_train, y_train_bin)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            f1 = f1_score(y_test_bin, y_pred_test)\n",
    "\n",
    "            results['c'].append(c)\n",
    "            results['solver'].append(solver)\n",
    "            results['penalty'].append(penalty)\n",
    "            results['F1'].append(f1)\n",
    "\n",
    "scores_df = pd.DataFrame(results, index=None)\n",
    "scores_df = scores_df.sort_values(by=\"F1\", ascending=False)\n",
    "\n",
    "best_c = scores_df.iloc[0].c # any\n",
    "best_solver = scores_df.iloc[0].solver # newton-cg/newton-cholesky\n",
    "best_penalty = scores_df.iloc[0].penalty # None\n",
    "\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "logistic_regression_model = LogisticRegression(C=best_c, solver=best_solver, penalty=best_penalty)\n",
    "get_final_model_scores(logistic_regression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We picked the Decision Tree model because it achieved better results\n",
    "get_final_model_scores(decision_tree_model, validation_set=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
